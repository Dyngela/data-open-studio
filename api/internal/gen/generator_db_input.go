package gen

import (
	"api/internal/api/models"
	"fmt"
)

// DBInputGenerator handles database input operations with pipeline streaming
type DBInputGenerator struct {
	BaseGenerator
	config models.DBInputConfig
}

// NewDBInputGenerator creates a new DB input generator
func NewDBInputGenerator(nodeID int, nodeName string, config models.DBInputConfig) *DBInputGenerator {
	return &DBInputGenerator{
		BaseGenerator: BaseGenerator{
			nodeID:   nodeID,
			nodeName: nodeName,
			nodeType: models.NodeTypeDBInput,
		},
		config: config,
	}
}

// GenerateFunctionSignature returns the function signature for streaming DB input
func (g *DBInputGenerator) GenerateFunctionSignature() string {
	nodeName := sanitizeNodeName(g.nodeName)
	// Returns a RowStream for pipeline processing
	return fmt.Sprintf("func executeNode_%d_%s(ctx *JobContext) *RowStream",
		g.nodeID, nodeName)
}

// GenerateFunctionBody returns the function body with channel-based streaming
func (g *DBInputGenerator) GenerateFunctionBody() string {
	g.config.EnforceSchema()
	connID := g.config.Connection.GetConnectionID()
	query := g.config.QueryWithSchema

	// Buffer size for the channel - balance between memory and throughput
	bufferSize := g.config.BatchSize
	if bufferSize <= 0 {
		bufferSize = 1000
	}

	return fmt.Sprintf(`	stream := NewRowStream(%d)

	go func() {
		defer stream.Close()

		// Get connection
		ctx.mu.RLock()
		db := ctx.Connections[%q]
		ctx.mu.RUnlock()

		if db == nil {
			stream.SendError(fmt.Errorf("connection %%s not found", %q))
			return
		}

		query := %q
		log.Printf("Node %d: Executing query: %%s", query)

		rows, err := db.Query(query)
		if err != nil {
			stream.SendError(fmt.Errorf("query failed: %%w", err))
			return
		}
		defer rows.Close()

		columns, err := rows.Columns()
		if err != nil {
			stream.SendError(fmt.Errorf("failed to get columns: %%w", err))
			return
		}

		colCount := len(columns)
		// Reuse scan buffer across all rows
		values := make([]interface{}, colCount)
		valuePtrs := make([]interface{}, colCount)
		for i := range values {
			valuePtrs[i] = &values[i]
		}

		rowCount := 0
		for rows.Next() {
			if err := rows.Scan(valuePtrs...); err != nil {
				stream.SendError(fmt.Errorf("scan failed at row %%d: %%w", rowCount, err))
				return
			}

			// Build row map with pre-allocated capacity
			row := make(map[string]interface{}, colCount)
			for i, col := range columns {
				val := values[i]
				if b, ok := val.([]byte); ok {
					// Copy bytes to avoid reference to reused buffer
					row[col] = string(b)
				} else {
					row[col] = val
				}
				values[i] = nil // Clear for GC
			}

			// Send to channel - blocks if buffer full (backpressure)
			if !stream.Send(row) {
				return // Context cancelled
			}
			rowCount++
		}

		if err := rows.Err(); err != nil {
			stream.SendError(fmt.Errorf("iteration error: %%w", err))
			return
		}

		log.Printf("Node %d: Streamed %%d rows", rowCount)
	}()

	return stream`, bufferSize, connID, connID, query, g.nodeID, g.nodeID)
}

// GenerateHelperFunctions returns the RowStream type and helpers
func (g *DBInputGenerator) GenerateHelperFunctions() string {
	// Only generate once - check if this is the first DBInput generator
	// The actual helper is generated by MainProgramGenerator
	return ""
}

func (g *DBInputGenerator) GenerateImports() []string {
	return []string{
		`"database/sql"`,
		`"fmt"`,
		`"context"`,
		`_ "` + g.config.Connection.GetImportPath() + `"`,
	}
}
